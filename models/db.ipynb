{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9e19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "db_path = \"../dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf286405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce30699",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d373a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{db_path}/train.json\", \"r\") as fp:\n",
    "    train = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff9754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pluto rotates once on its axis every 6.39 Earth days;',\n",
       " 'Earth rotates on its axis once times in one day.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['premise']['0'], train['hypothesis']['0'], train['label']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074eeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pluto rotates once on its axis every 6.39 Earth days;',\n",
       " ['pluto', 'rotates', 'axis', 'every', '639', 'earth', 'days'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    stem = False\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatize = False\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenize = True\n",
    "\n",
    "    clean_text = True\n",
    "    regex = r\"[^a-z0-9\\-\\s]\"\n",
    "\n",
    "    pos = False\n",
    "    pos_tagger = pos_tag\n",
    "\n",
    "    lower = True\n",
    "\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "class Embedder():\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.lemmatizer = cfg.lemmatizer\n",
    "        self.stemmer = cfg.stemmer\n",
    "        self.stop_words = cfg.stopwords\n",
    "\n",
    "\n",
    "    def embed(self, sentence: str) -> list[str]:\n",
    "        tokens = [sentence]\n",
    "        if self.cfg.tokenize:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        if self.cfg.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        if self.cfg.stem:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        if self.cfg.clean_text:\n",
    "                \n",
    "            tokens = [re.sub(self.cfg.regex, '', token.lower() if self.cfg.lower else token) for token in tokens]\n",
    "            tokens = [token for token in tokens if token and token not in self.stop_words]\n",
    "\n",
    "        if self.cfg.pos:\n",
    "            tokens = self.cfg.pos_tagger(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def embed_data(self, json):\n",
    "        len = len(json['label'])\n",
    "        print(len)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,  DataLoader\n",
    "\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "  def __init__(self, df, embed_model):\n",
    "    self.df = df\n",
    "    self.embed_model = embed_model\n",
    "\n",
    "    self.max_length = df['clean_text'].str.len().max()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    clean = self.df[\"clean_text\"].iloc[index]\n",
    "    embed = np.array([self.embed_model.wv[word] for word in clean if word in self.embed_model.wv])\n",
    "    label = int(self.df[\"condition_label\"].iloc[index]) - 1\n",
    "\n",
    "    embed = torch.tensor(embed, dtype=torch.float32)\n",
    "    label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    return embed, label\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "embedder = Embedder(Config)\n",
    "train['premise']['0'], embedder.embed(train['premise']['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class inferenceDataset(Dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
