{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 CITS4012 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_path = \"dataset\"\n",
        "\n",
        "train_file = f\"{db_path}/train.json\"\n",
        "test_file = f\"{db_path}/test.json\"\n",
        "val_file = f\"{db_path}/validation.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Pluto rotates once on its axis every 6.39 Earth days;',\n",
              " 'Earth rotates on its axis once times in one day.',\n",
              " 'neutral',\n",
              " 23088)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "with open(train_file, \"r\") as fp:\n",
        "    train = json.load(fp)\n",
        "with open(val_file, \"r\") as fp:\n",
        "    val = json.load(fp)\n",
        "with open(test_file, \"r\") as fp:\n",
        "    test = json.load(fp)\n",
        "\n",
        "train['premise']['0'], train['hypothesis']['0'], train['label']['0'], len(train['premise'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "from nltk import pos_tag\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Config:\n",
        "    stem = False\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatize = True\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenize = True\n",
        "\n",
        "    clean_text = True\n",
        "    regex = r\"[^a-z0-9\\-\\s]\"\n",
        "\n",
        "    pos = True\n",
        "    pos_tagger = pos_tag\n",
        "\n",
        "    lower = True\n",
        "\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class textProcesser():\n",
        "    def __init__(self, cfg: Config) -> None:\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.lemmatizer = cfg.lemmatizer\n",
        "        self.stemmer = cfg.stemmer\n",
        "        self.stop_words = cfg.stopwords\n",
        "\n",
        "        self.pos_tagger = cfg.pos_tagger\n",
        "\n",
        "\n",
        "    def clean(self, sentence: str) -> list[str]:\n",
        "\n",
        "        clean = {\n",
        "            'tokens': [],\n",
        "            'pos': [],\n",
        "            'segments': []\n",
        "        }\n",
        "        tokens = [sentence]\n",
        "\n",
        "        if self.cfg.tokenize:\n",
        "            tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "        if self.cfg.lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.stem:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.clean_text:\n",
        "            tokens = [re.sub(self.cfg.regex, '', token.lower() if self.cfg.lower else token) for token in tokens]\n",
        "            tokens = [token for token in tokens if token and token not in self.stop_words]\n",
        "\n",
        "        if self.cfg.pos:\n",
        "            tokens_pos = pos_tag(tokens)\n",
        "            tokens, pos_tags = zip(*tokens_pos) if tokens_pos else ([], [])\n",
        "            clean['pos'] = pos_tags\n",
        "            clean['tokens'] = tokens\n",
        "\n",
        "            \n",
        "\n",
        "        return clean\n",
        "    \n",
        "    def clean_text(self, json):\n",
        "        clean = {\n",
        "            'premise': {},\n",
        "            'premise_pos': {},\n",
        "            'hypothesis': {},\n",
        "            'hypothesis_pos': {},\n",
        "            'label': {}\n",
        "        }\n",
        "\n",
        "        for idx, key in enumerate(json['label'].keys()):\n",
        "            premise_clean = self.clean(json['premise'][key])\n",
        "            h_clean = self.clean(json['hypothesis'][key])\n",
        "\n",
        "            p_tokens, p_pos = premise_clean['tokens'], premise_clean['pos']\n",
        "            h_tokens, h_pos = h_clean['tokens'], h_clean['pos']\n",
        "\n",
        "            if len(p_tokens) == 0 or len(h_tokens) == 0:\n",
        "                print(f\" Warning: Empty premise or hypothesis at index {idx} (key: {key}).\")\n",
        "\n",
        "            clean['label'][idx] = 0 if 'neutral' in json['label'][key] else 1\n",
        "            clean['premise'][idx] = p_tokens\n",
        "            clean['premise_pos'][idx] = p_pos\n",
        "\n",
        "            if len(p_tokens) != len(p_pos):\n",
        "                raise ValueError(f\"Mismatch in premise tokens and POS tags at index {idx} (key: {key}). len(p_tokens): {len(p_tokens)}, len(p_pos): {len(p_pos)}\")\n",
        "            clean['hypothesis'][idx] = h_tokens\n",
        "            clean['hypothesis_pos'][idx] = h_pos\n",
        "\n",
        "        return clean\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Warning: Empty premise or hypothesis at index 146 (key: 146).\n"
          ]
        }
      ],
      "source": [
        "cfg = Config()\n",
        "\n",
        "clean_train = textProcesser(cfg).clean_text(train)\n",
        "clean_val = textProcesser(cfg).clean_text(val)\n",
        "clean_test = textProcesser(cfg).clean_text(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([],\n",
              " ('take', 'earth', 'one', 'week', 'rotate', 'axis', 'seven', 'time'),\n",
              " 0,\n",
              " [],\n",
              " ('VB', 'NN', 'CD', 'NN', 'NN', 'VBD', 'CD', 'NN'))"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_train['premise'][146], clean_train['hypothesis'][146], clean_train['label'][146], clean_train['premise_pos'][146], clean_train['hypothesis_pos'][146]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label                                                          0\n",
              "premise           (pluto, rotates, axis, every, 639, earth, day)\n",
              "hypothesis                (earth, rotates, axis, time, one, day)\n",
              "premise_pos                       (NN, NNS, VBP, DT, CD, NN, NN)\n",
              "hypothesis_pos                         (NN, VBZ, JJ, NN, CD, NN)\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame({\n",
        "    \"label\": clean_train[\"label\"],\n",
        "    \"premise\": clean_train[\"premise\"],\n",
        "    \"hypothesis\": clean_train[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_train[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_train[\"hypothesis_pos\"],\n",
        "})\n",
        "val_df = pd.DataFrame({\n",
        "    \"label\": clean_val[\"label\"],\n",
        "    \"premise\": clean_val[\"premise\"],\n",
        "    \"hypothesis\": clean_val[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_val[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_val[\"hypothesis_pos\"],\n",
        "\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"label\": clean_test[\"label\"],\n",
        "    \"premise\": clean_test[\"premise\"],\n",
        "    \"hypothesis\": clean_test[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_test[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_test[\"hypothesis_pos\"],\n",
        "})\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "splits = [('Train', train_df), ('Validation', val_df), ('Test', test_df)]\n",
        "colors = ['skyblue', 'salmon', 'lightgreen']  # one color per split\n",
        "\n",
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "for i, ((name, df), color) in enumerate(zip(splits, colors), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    counts = df['label'].value_counts().sort_index()\n",
        "    sns.barplot(x=['Neutral', 'Entails'], y=counts.values, color=color)\n",
        "    plt.title(f\"{name} Set\")\n",
        "    plt.xlabel(\"Label\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build pos2idx from training set only\n",
        "all_pos_tags = set()\n",
        "for pos_list in train_df['premise_pos']:\n",
        "    all_pos_tags.update(pos_list)\n",
        "for pos_list in train_df['hypothesis_pos']:\n",
        "    all_pos_tags.update(pos_list)\n",
        "\n",
        "# Add a special <UNK> token\n",
        "pos2idx = {pos: i for i, pos in enumerate(sorted(all_pos_tags))}\n",
        "pos2idx['<UNK>'] = len(pos2idx)\n",
        "num_pos_tags = len(pos2idx)\n",
        "\n",
        "def encode_pos(pos_list, pos2idx):\n",
        "    return [pos2idx.get(pos, pos2idx['<UNK>']) for pos in pos_list]\n",
        "\n",
        "\n",
        "len(pos2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = list(clean_train['premise'].values()) + list(clean_train['hypothesis'].values())\n",
        "embed_model = Word2Vec(sentences=sentences, vector_size=200, window=10, min_count=3, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,  DataLoader\n",
        "\n",
        "class nliDataset(Dataset):\n",
        "  def __init__(self, df, embed_model, pos_embed_model=None, pos2idx=pos2idx, encode_pos=encode_pos):\n",
        "    self.df = df\n",
        "    self.embed_model = embed_model\n",
        "    self.pos_encode = encode_pos\n",
        "    self.pos2idx = pos2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    premise = self.df[\"premise\"].iloc[index]\n",
        "    hypothesis = self.df[\"hypothesis\"].iloc[index]\n",
        "    \n",
        "    p_embed = np.array([\n",
        "        self.embed_model.wv[word] if word in self.embed_model.wv else np.zeros(self.embed_model.vector_size)\n",
        "        for word in premise\n",
        "    ])\n",
        "    h_embed = np.array([\n",
        "        self.embed_model.wv[word] if word in self.embed_model.wv else np.zeros(self.embed_model.vector_size)\n",
        "        for word in hypothesis\n",
        "    ])\n",
        "\n",
        "\n",
        "    if len(p_embed) == 0:\n",
        "        p_embed = np.zeros((1, self.embed_model.vector_size))\n",
        "    if len(h_embed) == 0:\n",
        "        h_embed = np.zeros((1, self.embed_model.vector_size))\n",
        "    \n",
        "\n",
        "    premise_pos = self.df[\"premise_pos\"].iloc[index]\n",
        "    hypothesis_pos = self.df[\"hypothesis_pos\"].iloc[index]\n",
        "\n",
        "    premise_pos = self.pos_encode(premise_pos, self.pos2idx)\n",
        "    hypothesis_pos = self.pos_encode(hypothesis_pos, self.pos2idx)\n",
        "\n",
        "    # If embeddings were padded to length 1, also pad POS\n",
        "    if len(p_embed) == 1 and len(premise) == 0:\n",
        "        premise_pos = [self.pos2idx['<UNK>']]\n",
        "    if len(h_embed) == 1 and len(hypothesis) == 0:\n",
        "        hypothesis_pos = [self.pos2idx['<UNK>']]\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    if len(premise_pos) != len(p_embed):\n",
        "        raise ValueError(f\"Mismatch in lengths at index {index}. premise_pos length: {len(premise_pos)}, p_embed length: {len(p_embed)}\")\n",
        "\n",
        "\n",
        "    label = int(self.df[\"label\"].iloc[index])\n",
        "    p_embed = torch.tensor(p_embed, dtype=torch.float32)\n",
        "    premise_pos = torch.tensor(premise_pos, dtype=torch.long)\n",
        "\n",
        "    h_embed = torch.tensor(h_embed, dtype=torch.float32)\n",
        "    hypothesis_pos = torch.tensor(hypothesis_pos, dtype=torch.long)\n",
        "\n",
        "    label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
        "    p_seg = torch.tensor(0, dtype=torch.long).unsqueeze(0)\n",
        "    h_seg = torch.tensor(1, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    \n",
        "    \n",
        "    sample = {\n",
        "        \"p_embed\": p_embed,\n",
        "        \"h_embed\": h_embed,\n",
        "        \"label\": label,\n",
        "        \"premise_pos\": premise_pos,\n",
        "        \"hypothesis_pos\": hypothesis_pos,\n",
        "        \"premise_segments\": p_seg,\n",
        "        \"hypothesis_segments\": h_seg,\n",
        "    }\n",
        "\n",
        "    return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'p_embed': tensor([[ 0.0393, -0.0446, -0.2052,  ..., -0.0653,  0.2923, -0.1767],\n",
              "         [ 0.2034, -0.1662, -0.3517,  ..., -0.0573,  0.3662, -0.1239],\n",
              "         [ 0.1116,  0.0587, -0.4929,  ..., -0.0066,  0.1132, -0.1983],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
              "         [ 0.1779, -0.1923, -0.5290,  ..., -0.1347,  0.2418, -0.1905]]),\n",
              " 'h_embed': tensor([[ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
              "         [ 0.2034, -0.1662, -0.3517,  ..., -0.0573,  0.3662, -0.1239],\n",
              "         [ 0.1116,  0.0587, -0.4929,  ..., -0.0066,  0.1132, -0.1983],\n",
              "         [ 0.2147,  0.2590, -0.1726,  ...,  0.0369,  0.0336, -0.1702],\n",
              "         [-0.4303,  0.1614, -0.0604,  ...,  0.0351, -0.0720,  0.1962],\n",
              "         [ 0.1779, -0.1923, -0.5290,  ..., -0.1347,  0.2418, -0.1905]]),\n",
              " 'label': tensor([0.]),\n",
              " 'premise_pos': tensor([13, 15, 30,  5,  4, 13, 13]),\n",
              " 'hypothesis_pos': tensor([13, 31,  9, 13,  4, 13]),\n",
              " 'premise_segments': tensor(1),\n",
              " 'hypothesis_segments': tensor(2)}"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db = nliDataset(train_df, embed_model)\n",
        "db.__getitem__(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2886, 163, 266)"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    p_embeds = [item[\"p_embed\"] for item in batch]\n",
        "    h_embeds = [item[\"h_embed\"] for item in batch]\n",
        "    p_pos_idx = [item[\"premise_pos\"] for item in batch]\n",
        "    h_pos_idx = [item[\"hypothesis_pos\"] for item in batch]\n",
        "\n",
        "    p_segments = [item[\"premise_segments\"] for item in batch]\n",
        "    h_segments = [item[\"hypothesis_segments\"] for item in batch]\n",
        "    labels = [item[\"label\"] for item in batch]\n",
        "\n",
        "\n",
        "    p_padded = pad_sequence(p_embeds, batch_first=True) \n",
        "    h_padded = pad_sequence(h_embeds, batch_first=True) \n",
        "    p_pos_padded = pad_sequence(p_pos_idx, batch_first=True, padding_value=pos2idx['<UNK>'])\n",
        "    h_pos_padded = pad_sequence(h_pos_idx, batch_first=True, padding_value=pos2idx['<UNK>'])\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    p_seg = torch.stack(p_segments)\n",
        "    h_seg = torch.stack(h_segments)\n",
        "\n",
        "    plengths = torch.tensor([x.shape[0] for x in p_embeds])\n",
        "    hlengths = torch.tensor([x.shape[0] for x in h_embeds])\n",
        "\n",
        "\n",
        "    if p_pos_padded.shape[1] != p_padded.shape[1]:\n",
        "        raise ValueError(\"Mismatch in padded lengths between p_pos and p_embed.\")\n",
        "    return {\n",
        "        \"p_embed\": p_padded,\n",
        "        \"h_embed\": h_padded,\n",
        "        \"p_pos\": p_pos_padded,\n",
        "        \"h_pos\": h_pos_padded,\n",
        "        \"label\": labels,\n",
        "        \"p_lengths\": plengths,\n",
        "        \"h_lengths\": hlengths,\n",
        "        \"p_segments\": p_seg,\n",
        "        \"h_segments\": h_seg\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "train_db = nliDataset(train_df, embed_model=embed_model)\n",
        "trainloader = DataLoader(train_db, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "val_db = nliDataset(val_df, embed_model=embed_model)\n",
        "valloader = DataLoader(val_db, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "test_db = nliDataset(test_df, embed_model=embed_model)\n",
        "testloader = DataLoader(test_db, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "len(trainloader), len(valloader), len(testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p_embed: tensor([[[ 0.0393, -0.0446, -0.2052,  ..., -0.0653,  0.2923, -0.1767],\n",
            "         [ 0.2034, -0.1662, -0.3517,  ..., -0.0573,  0.3662, -0.1239],\n",
            "         [ 0.1116,  0.0587, -0.4929,  ..., -0.0066,  0.1132, -0.1983],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0356, -0.3345, -0.1316,  ...,  0.0499,  0.0370, -0.3361],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.5305, -0.0482, -0.2284,  ..., -0.3207, -0.4024, -0.3470],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.1904,  0.0721, -0.1574,  ..., -0.3276, -0.0345, -0.0793],\n",
            "         [-0.0847, -0.2717, -0.0088,  ..., -0.0102,  0.2952,  0.0780],\n",
            "         [-0.4052, -0.1907, -0.0680,  ..., -0.6249,  0.3885,  0.1630],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0393, -0.0446, -0.2052,  ..., -0.0653,  0.2923, -0.1767],\n",
            "         [ 0.0903, -0.1015, -0.1197,  ..., -0.1007,  0.0556, -0.0276],\n",
            "         [ 0.2147,  0.2590, -0.1726,  ...,  0.0369,  0.0336, -0.1702],\n",
            "         ...,\n",
            "         [ 0.1410, -0.0983, -0.1715,  ..., -0.1728,  0.4166, -0.0840],\n",
            "         [-0.2326, -0.0183, -0.3502,  ..., -0.1618,  0.4689, -0.0645],\n",
            "         [ 0.5294, -0.2495, -0.7456,  ..., -0.1237, -0.0844, -0.3087]],\n",
            "\n",
            "        [[ 0.0081,  0.0959, -0.1305,  ..., -0.2287,  0.1586, -0.4540],\n",
            "         [ 0.0393, -0.0446, -0.2052,  ..., -0.0653,  0.2923, -0.1767],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.3604, -0.1166, -0.3215,  ...,  0.1789, -0.0348,  0.0057],\n",
            "         [-0.5698, -0.3651,  0.0859,  ..., -0.2040,  0.3352,  0.1097],\n",
            "         [-0.4172,  0.0056, -0.3076,  ..., -0.4889,  0.2267,  0.0994],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
            "h_embed: tensor([[[ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
            "         [ 0.2034, -0.1662, -0.3517,  ..., -0.0573,  0.3662, -0.1239],\n",
            "         [ 0.1116,  0.0587, -0.4929,  ..., -0.0066,  0.1132, -0.1983],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
            "         [ 0.2034, -0.1662, -0.3517,  ..., -0.0573,  0.3662, -0.1239],\n",
            "         [ 0.1116,  0.0587, -0.4929,  ..., -0.0066,  0.1132, -0.1983],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0462,  0.2885,  0.1171,  ..., -0.1985, -0.2051, -0.0807],\n",
            "         [ 0.5294, -0.2495, -0.7456,  ..., -0.1237, -0.0844, -0.3087],\n",
            "         [-0.1026, -0.0364, -0.0830,  ..., -0.3014, -0.0265, -0.3492],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
            "         [ 0.3994,  0.3622,  0.1506,  ...,  0.0429,  0.8073,  0.0189],\n",
            "         [-0.0193, -0.2214,  0.0783,  ..., -0.0724,  0.3861, -0.0411],\n",
            "         ...,\n",
            "         [ 0.2249, -0.1860, -0.0041,  ..., -0.0071,  0.3013, -0.0387],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3083, -0.2020, -0.4449,  ..., -0.0050,  0.3945,  0.0448],\n",
            "         [ 0.3994,  0.3622,  0.1506,  ...,  0.0429,  0.8073,  0.0189],\n",
            "         [-0.0193, -0.2214,  0.0783,  ..., -0.0724,  0.3861, -0.0411],\n",
            "         ...,\n",
            "         [ 0.2249, -0.1860, -0.0041,  ..., -0.0071,  0.3013, -0.0387],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.3902, -0.2560, -0.3914,  ..., -0.0070,  0.2703, -0.1161],\n",
            "         [-0.5698, -0.3651,  0.0859,  ..., -0.2040,  0.3352,  0.1097],\n",
            "         [-0.3604, -0.1166, -0.3215,  ...,  0.1789, -0.0348,  0.0057],\n",
            "         ...,\n",
            "         [-0.2572, -0.2405, -0.2095,  ..., -0.2121, -0.0042, -0.1167],\n",
            "         [-0.2118, -0.0990, -0.0951,  ..., -0.2277, -0.0992, -0.2945],\n",
            "         [-0.2271, -0.1912,  0.0531,  ..., -0.1850,  0.0962, -0.0333]]])\n",
            "p_pos: tensor([[13, 15, 30,  5,  4, 13, 13, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36],\n",
            "        [ 2, 13,  8, 13, 13, 31,  8, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36],\n",
            "        [23,  2,  9, 13,  9, 13, 13, 13, 36, 36, 36, 36, 36, 36, 36, 36, 36],\n",
            "        [15,  9, 13, 13, 27,  9, 13, 13, 13, 29, 13, 36, 36, 36, 36, 36, 36],\n",
            "        [13, 13, 31,  9,  8, 13, 31,  8, 13,  8, 13, 36, 36, 36, 36, 36, 36],\n",
            "        [13,  4, 13,  9, 13, 13, 26,  4,  9, 13, 19, 26,  4,  9, 13,  8, 13],\n",
            "        [26, 13,  4, 13, 13, 30,  4, 13,  8, 13, 36, 36, 36, 36, 36, 36, 36],\n",
            "        [ 9, 13,  9, 13,  9, 13, 13,  9, 13, 36, 36, 36, 36, 36, 36, 36, 36]])\n",
            "h_pos: tensor([[13, 31,  9, 13,  4, 13, 36, 36, 36, 36],\n",
            "        [13, 31,  9, 13,  4, 13, 36, 36, 36, 36],\n",
            "        [13, 13,  9, 13, 19, 13,  9, 36, 36, 36],\n",
            "        [13,  9, 28,  9, 13, 13, 13, 36, 36, 36],\n",
            "        [13, 31,  9, 13,  4, 13, 36, 36, 36, 36],\n",
            "        [ 9, 26,  9, 13,  8, 13,  5, 13, 36, 36],\n",
            "        [ 9, 26,  9, 13,  8, 13,  5, 13, 36, 36],\n",
            "        [ 9, 13,  9, 13,  9, 13, 26,  9, 13, 13]])\n",
            "label: tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "p_lengths: tensor([ 7,  7,  8, 11, 11, 17, 10,  9])\n",
            "h_lengths: tensor([ 6,  6,  7,  7,  6,  8,  8, 10])\n",
            "p_segments: [tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
            "h_segments: [tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2)]\n"
          ]
        }
      ],
      "source": [
        "out = next(iter(trainloader))\n",
        "for key, value in out.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "class Task():\n",
        "  def __init__(self, model, \n",
        "                train_df, \n",
        "                val_df, \n",
        "                save_path, \n",
        "                collate_fn=collate_fn, \n",
        "                optimizer=None,\n",
        "                loss_fn=None,\n",
        "                device=\"cpu\", \n",
        "                batch_size=16, \n",
        "                weighted_sampling=False,\n",
        "                forward_func=None) -> None:\n",
        "\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "    self.forward = forward_func if forward_func is not None else model.forward\n",
        "\n",
        "    self.save_path = save_path\n",
        "\n",
        "    if weighted_sampling:\n",
        "      labels = train_df['label'].values\n",
        "      class_counts = Counter(labels)\n",
        "      num_samples = len(labels)\n",
        "      class_weights = {cls: num_samples/count for cls, count in class_counts.items()}\n",
        "      sample_weights = [class_weights[label] for label in labels]\n",
        "      sampler = WeightedRandomSampler(sample_weights, num_samples=num_samples, replacement=True)\n",
        "    else:\n",
        "      sampler = None\n",
        "    \n",
        "    train_db = nliDataset(train_df, embed_model=embed_model)\n",
        "    val_db = nliDataset(val_df, embed_model=embed_model)\n",
        "\n",
        "    self.train_loader = DataLoader(train_db, batch_size=batch_size, sampler=sampler, shuffle=True, collate_fn=collate_fn)\n",
        "    self.val_loader = DataLoader(val_db, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    self.model = model\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "    self.model = model.to(device)\n",
        "\n",
        "    self.losses = {}\n",
        "    self.losses[\"train\"] = []\n",
        "    self.losses[\"val\"] = []\n",
        "\n",
        "  def train_epoch(self):\n",
        "    self.model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(self.train_loader, total=len(self.train_loader), desc=\"Training\"):\n",
        "        \n",
        "        labels = batch[\"label\"]\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        outputs = self.forward(batch, self.model, self.device)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(self.train_loader)\n",
        "    return epoch_loss\n",
        "  \n",
        "\n",
        "  def validate_epoch(self):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, total=len(self.val_loader), desc=\"Validating\"):\n",
        "            \n",
        "          labels = batch[\"label\"]\n",
        "          labels = labels.to(self.device)\n",
        "\n",
        "          outputs = self.forward(batch, self.model, self.device)\n",
        "          loss = self.loss_fn(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "\n",
        "          predicted = (torch.sigmoid(outputs) > 0.5).long().squeeze()  # shape [batch_size]\n",
        "          correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "          total += labels.size(0)\n",
        "\n",
        "        val_loss /= len(self.val_loader)\n",
        "        accuracy = correct / total\n",
        "    return val_loss, accuracy\n",
        "     \n",
        "\n",
        "  def fit(self, epochs=10, restore_best=True, patience=3):\n",
        "    count = 0\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = self.train_epoch()\n",
        "        val_loss, val_acc = self.validate_epoch()\n",
        "\n",
        "        self.losses[\"train\"].append(train_loss)\n",
        "        self.losses[\"val\"].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            count = 0\n",
        "            torch.save(self.model.state_dict(), self.save_path)\n",
        "            print(f\"Model saved to {self.save_path}\")\n",
        "\n",
        "        count += 1\n",
        "        if count >= patience:\n",
        "            print(f\"No improvement in validation loss for {patience} consecutive epochs. Early stopping.\")\n",
        "            break\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    if restore_best:\n",
        "      print(\"Restoring best model weights.\")\n",
        "      self.model.load_state_dict(torch.load(self.save_path))\n",
        "    return self.losses\n",
        "  \n",
        "\n",
        "  def plot_losses(self):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(self.losses[\"train\"], label=\"Train Loss\")\n",
        "    plt.plot(self.losses[\"val\"], label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def visualize_predictions(self, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualizes predictions vs targets for a few random samples from the validation set.\n",
        "\n",
        "    Args:\n",
        "        task: Your Task object (with .model and .val_loader)\n",
        "        num_samples: Number of random examples to show\n",
        "    \"\"\"\n",
        "    self.model.eval()\n",
        "    all_samples = []\n",
        "\n",
        "    # Collect a few batches\n",
        "    with torch.no_grad():\n",
        "        for batch in self.val_loader:\n",
        "            labels = batch[\"label\"].to(self.device)\n",
        "            outputs = self.forward(batch, self.model, self.device)\n",
        "            probs = torch.sigmoid(outputs.squeeze())\n",
        "\n",
        "            for prob, label in zip(probs, labels):\n",
        "                pred_label = int(prob > 0.5)\n",
        "                all_samples.append((prob.item(), pred_label, int(label.item())))\n",
        "\n",
        "    samples_to_show = random.sample(all_samples, min(num_samples, len(all_samples)))\n",
        "\n",
        "    print(f\"{'Pred Prob':>10} | {'Pred Label':>10} | {'Target':>6}\")\n",
        "    print(\"-\"*32)\n",
        "    for prob, pred, target in samples_to_show:\n",
        "        print(f\"{prob:10.3f} | {pred:10} | {target:6}\")\n",
        "\n",
        "  def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the validation set and return classification metrics.\n",
        "        Returns:\n",
        "            dict: Contains accuracy, precision, recall, f1, confusion matrix, and full report\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, total=len(self.val_loader), desc=\"Evaluating\"):\n",
        "                outputs = self.forward(batch, self.model, self.device)\n",
        "                probs = torch.sigmoid(outputs.squeeze())\n",
        "                preds = (probs > 0.5).long()\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy().tolist())\n",
        "                all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(all_labels, all_preds),\n",
        "            \"precision\": precision_score(all_labels, all_preds, zero_division=0),\n",
        "            \"recall\": recall_score(all_labels, all_preds, zero_division=0),\n",
        "            \"f1\": f1_score(all_labels, all_preds, zero_division=0),\n",
        "            \"confusion_matrix\": confusion_matrix(all_labels, all_preds).tolist(),\n",
        "            \"classification_report\": classification_report(all_labels, all_preds, zero_division=0, output_dict=True)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "  \n",
        "\n",
        "  def visualize_metrcis(self, metrics_dict, class_names=['Neutral', 'Entails']):\n",
        "      \"\"\"\n",
        "      Visualize metrics, confusion matrix, and classification report from a precomputed metrics dictionary.\n",
        "\n",
        "      Args:\n",
        "          metrics_dict (dict): Dictionary containing 'accuracy', 'precision', 'recall', 'f1',\n",
        "                              'confusion_matrix' (as list of lists), and 'classification_report' (as dict)\n",
        "          class_names (list, optional): Class names for confusion matrix\n",
        "      \"\"\"\n",
        "\n",
        "      # 1. Display main metrics\n",
        "      print(\"=== Overall Metrics ===\")\n",
        "      for key in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
        "          print(f\"{key.capitalize():<10}: {metrics_dict[key]:.4f}\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "      # 2. Confusion Matrix\n",
        "      cm = metrics_dict[\"confusion_matrix\"]\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                  xticklabels=class_names, yticklabels=class_names)\n",
        "      plt.xlabel('Predicted')\n",
        "      plt.ylabel('Actual')\n",
        "      plt.title('Confusion Matrix')\n",
        "      plt.show()\n",
        "\n",
        "      # 3. Classification Report\n",
        "      report_dict = metrics_dict[\"classification_report\"]\n",
        "      report_df = pd.DataFrame(report_dict).transpose()\n",
        "      print(\"=== Classification Report ===\\n\")\n",
        "      display(report_df)  # nicer tabular display in notebooks\n",
        "\n",
        "      # Optional: bar plot for main metrics\n",
        "      plt.figure(figsize=(5, 4))\n",
        "      main_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "      values = [metrics_dict[m] for m in main_metrics]\n",
        "      sns.barplot(x=main_metrics, y=values, palette=\"viridis\")\n",
        "      plt.ylim(0, 1)\n",
        "      plt.title(\"Main Metrics\")\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class twin_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=100, hidden_size=128, output_size=1, \n",
        "                 dropout=0.5, bidirectional=True, n_layers=3,\n",
        "                 pos_vocab_size=None, pos_emb_dim=16):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Word + POS embeddings combined\n",
        "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_emb_dim) if pos_vocab_size else None\n",
        "        combined_input_size = input_size + (pos_emb_dim if pos_vocab_size else 0)\n",
        "\n",
        "        self.px_lstm = nn.LSTM(combined_input_size, self.hidden_size, \n",
        "                               num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hx_lstm = nn.LSTM(combined_input_size, self.hidden_size, \n",
        "                               num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size * 2 * (2 if bidirectional else 1), output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, px, hx, plengths=None, hlengths=None, p_pos_idx=None, h_pos_idx=None):\n",
        "        # Add POS embeddings if provided\n",
        "        if self.pos_embedding is not None and p_pos_idx is not None and h_pos_idx is not None:\n",
        "            p_pos_emb = self.pos_embedding(p_pos_idx)\n",
        "            h_pos_emb = self.pos_embedding(h_pos_idx)\n",
        "\n",
        "\n",
        "            px = torch.cat([px, p_pos_emb], dim=2)\n",
        "            hx = torch.cat([hx, h_pos_emb], dim=2)\n",
        "\n",
        "        # Pack sequences\n",
        "        packed_px = pack_padded_sequence(px, plengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        px_output, (px_hn, _) = self.px_lstm(packed_px)\n",
        "\n",
        "        packed_hx = pack_padded_sequence(hx, hlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        hx_output, (hx_hn, _) = self.hx_lstm(packed_hx)\n",
        "\n",
        "        # Take last hidden state(s)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            px_out = torch.cat([px_hn[-2], px_hn[-1]], dim=1)\n",
        "            hx_out = torch.cat([hx_hn[-2], hx_hn[-1]], dim=1)\n",
        "        else:\n",
        "            px_out = px_hn[-1]\n",
        "            hx_out = hx_hn[-1]\n",
        "\n",
        "        out = self.fc(torch.cat([px_out, hx_out], dim=1))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pos_vocab_size = len(pos2idx) \n",
        "pos_emb_dim = 32              \n",
        "\n",
        "model = twin_LSTM(\n",
        "    input_size=embed_model.vector_size,\n",
        "    hidden_size=128,\n",
        "    output_size=1,\n",
        "    dropout=0.5,\n",
        "    bidirectional=True,\n",
        "    n_layers=2,\n",
        "    pos_vocab_size=0,\n",
        "    pos_emb_dim=pos_emb_dim\n",
        ")\n",
        "\n",
        "def twin_forward(batch, model, device):\n",
        "    p_embed = batch[\"p_embed\"].to(device)\n",
        "    h_embed = batch[\"h_embed\"].to(device)\n",
        "\n",
        "    p_lengths = batch[\"p_lengths\"]\n",
        "    h_lengths = batch[\"h_lengths\"]\n",
        "    premise_pos = batch[\"p_pos\"]\n",
        "    hypothesis_pos = batch[\"h_pos\"]\n",
        "\n",
        "    outputs = model(p_embed, h_embed, p_lengths, h_lengths, premise_pos, hypothesis_pos)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "twin_task = Task(\n",
        "    model=model,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    save_path=\"whatever.pth\",\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
        "    loss_fn=nn.BCEWithLogitsLoss(),\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    batch_size=8    ,\n",
        "    weighted_sampling=False,\n",
        "    forward_func=twin_forward\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  45%|     | 1285/2886 [01:14<01:32, 17.32it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mTask.fit\u001b[39m\u001b[34m(self, epochs, restore_best, patience)\u001b[39m\n\u001b[32m    128\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     val_loss, val_acc = \u001b[38;5;28mself\u001b[39m.validate_epoch()\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.losses[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mTask.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m h_padded = h_padded.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     72\u001b[39m labels = labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpremise_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_fn(outputs, labels)\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtwin_LSTM.forward\u001b[39m\u001b[34m(self, px, hx, plengths, hlengths, p_pos_idx, h_pos_idx)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Pack sequences\u001b[39;00m\n\u001b[32m     36\u001b[39m packed_px = pack_padded_sequence(px, plengths.cpu(), batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m px_output, (px_hn, _) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpx_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_px\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m packed_hx = pack_padded_sequence(hx, hlengths.cpu(), batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     40\u001b[39m hx_output, (hx_hn, _) = \u001b[38;5;28mself\u001b[39m.hx_lstm(packed_hx)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1136\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m     result = _VF.lstm(\n\u001b[32m   1125\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1126\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1133\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1134\u001b[39m     )\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1148\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "twin_task.fit(epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bert Inspired Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The BERT model from Devlin et al's 2018 paper implements a summation embedding of token, segment and position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_truncate(premises, hypotheses, max_len):\n",
        "    \"\"\"\n",
        "    Randomly truncate each premise/hypothesis pair in the batch to fit max_len.\n",
        "    \n",
        "    premises: list of tensors [(p_len_i, dim), ...] or tensor (b, p_len, dim)\n",
        "    hypotheses: list of tensors [(h_len_i, dim), ...] or tensor (b, h_len, dim)\n",
        "    Returns:\n",
        "        truncated_premises, truncated_hypotheses: lists of tensors\n",
        "    \"\"\"\n",
        "    truncated_premises = []\n",
        "    truncated_hypotheses = []\n",
        "\n",
        "    for p, h in zip(premises, hypotheses):\n",
        "        p_len, h_len = p.size(0), h.size(0)\n",
        "        total_len = p_len + h_len\n",
        "\n",
        "        while total_len > max_len:\n",
        "            if p_len > h_len and p_len > 1:\n",
        "                if random.random() < 0.5:\n",
        "                    p = p[1:]\n",
        "                else:\n",
        "                    p = p[:-1]\n",
        "                p_len -= 1\n",
        "            elif h_len > 1:\n",
        "                if random.random() < 0.5:\n",
        "                    h = h[1:]\n",
        "                else:\n",
        "                    h = h[:-1]\n",
        "                h_len -= 1\n",
        "            elif p_len > 1:\n",
        "                if random.random() < 0.5:\n",
        "                    p = p[1:]\n",
        "                else:\n",
        "                    p = p[:-1]\n",
        "                p_len -= 1\n",
        "            total_len = p_len + h_len\n",
        "\n",
        "        truncated_premises.append(p)\n",
        "        truncated_hypotheses.append(h)\n",
        "\n",
        "    return truncated_premises, truncated_hypotheses\n",
        "\n",
        "\n",
        "def batch_pad(truncated_premises, truncated_hypotheses, max_len, premise_seg_id=1, hypothesis_seg_id=2, pad_value=0.0):\n",
        "    \"\"\"\n",
        "    Pad each truncated sequence to max_len and create segments and attention masks.\n",
        "    Returns:\n",
        "        padded_seqs: (b, max_len, dim)\n",
        "        segments: (b, max_len)\n",
        "        attn_masks: (b, max_len)\n",
        "    \"\"\"\n",
        "    embed_dim = truncated_premises[0].size(1)\n",
        "\n",
        "    padded_seqs = []\n",
        "    segments_batch = []\n",
        "    attn_masks_batch = []\n",
        "\n",
        "    for p, h in zip(truncated_premises, truncated_hypotheses):\n",
        "        p_len, h_len = p.size(0), h.size(0)\n",
        "        seq = torch.cat([p, h], dim=0)\n",
        "        pad_len = max_len - seq.size(0)\n",
        "        if pad_len > 0:\n",
        "            seq = torch.cat([seq, torch.full((pad_len, embed_dim), pad_value)], dim=0)\n",
        "\n",
        "        segments = torch.cat([\n",
        "            torch.full((p_len,), premise_seg_id, dtype=torch.long),\n",
        "            torch.full((h_len,), hypothesis_seg_id, dtype=torch.long),\n",
        "            torch.full((pad_len,), 0, dtype=torch.long)\n",
        "        ])\n",
        "\n",
        "        attn_mask = torch.cat([\n",
        "            torch.ones(p_len + h_len, dtype=torch.long),\n",
        "            torch.zeros(pad_len, dtype=torch.long)\n",
        "        ])\n",
        "\n",
        "        padded_seqs.append(seq)\n",
        "        segments_batch.append(segments)\n",
        "        attn_masks_batch.append(attn_mask)\n",
        "\n",
        "    padded_seqs = torch.stack(padded_seqs)\n",
        "    segments_batch = torch.stack(segments_batch)\n",
        "    attn_masks_batch = torch.stack(attn_masks_batch)\n",
        "\n",
        "    return padded_seqs, segments_batch, attn_masks_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 20, 200]) torch.Size([8, 20]) torch.Size([8, 20])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(trainloader))\n",
        "premises = batch['p_embed']\n",
        "hypotheses = batch['h_embed']\n",
        "# Step 1: Truncate\n",
        "truncated_premises, truncated_hypotheses = batch_truncate(premises, hypotheses, max_len=20)\n",
        "\n",
        "# Step 2: Pad\n",
        "padded_seqs, segments, attn_masks = batch_pad(truncated_premises, truncated_hypotheses, max_len=20)\n",
        "\n",
        "print(padded_seqs.shape, segments.shape, attn_masks.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 128, 200]) torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BERTEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_segments,\n",
        "                 max_len,\n",
        "                 embed_dim,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.seg_embed = nn.Embedding(n_segments, embed_dim)\n",
        "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "\n",
        "\n",
        "        seg_embed = self.seg_embed(seg)\n",
        "\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)  # (batch, seq_len)\n",
        "        pos_embed = self.pos_embed(pos_ids)  \n",
        "\n",
        "        embed_val = x + seg_embed + pos_embed\n",
        "        embed_val = self.drop(embed_val)\n",
        "        return embed_val\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_segments,\n",
        "                 max_len,\n",
        "                 embed_dim,\n",
        "                 n_layers,\n",
        "                 attn_heads,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = BERTEmbedding(n_segments, max_len, embed_dim, dropout)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, attn_heads, embed_dim*4, batch_first=True)\n",
        "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, 32)\n",
        "            , nn.ReLU()\n",
        "            , nn.Dropout(dropout)\n",
        "            , nn.Linear(32, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, seq, attention_mask=None):\n",
        "        x = self.embedding(x, seg)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # TransformerEncoder expects mask of shape (batch_size, seq_len) -> key_padding_mask\n",
        "            # True means **ignore** (mask out), False means attend\n",
        "            key_padding_mask = attention_mask == 0\n",
        "        else:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        out = self.encoder_block(x, src_key_padding_mask=key_padding_mask)\n",
        "        pooled = (out * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N_SEGMENTS = 3\n",
        "    MAX_LEN = 128\n",
        "    EMBED_DIM = 200\n",
        "    N_LAYERS = 12\n",
        "    ATTN_HEADS = 10\n",
        "    DROPOUT = 0.1\n",
        "\n",
        "    batch = next(iter(trainloader))\n",
        "\n",
        "    premise = batch[\"p_embed\"]\n",
        "    hypothesis = batch[\"h_embed\"]\n",
        "    p_seg = batch[\"p_segments\"]\n",
        "    h_seg = batch[\"h_segments\"]\n",
        "\n",
        "\n",
        "    p, h = batch_truncate(premise, hypothesis, max_len=MAX_LEN)\n",
        "    x, seg, attn_mask = batch_pad(p, h, max_len=MAX_LEN)\n",
        "    \n",
        "    print(x.size(), seg.size(), attn_mask.size())\n",
        "\n",
        "    embedding = BERT(\n",
        "        n_segments=N_SEGMENTS,\n",
        "        max_len=MAX_LEN,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        n_layers=N_LAYERS,\n",
        "        attn_heads=ATTN_HEADS,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "    embedding.eval()\n",
        "    out = embedding(x, seg, attn_mask)\n",
        "\n",
        "    print(out.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_model = BERT(\n",
        "    n_segments=3,\n",
        "    max_len=128,\n",
        "    embed_dim=embed_model.vector_size,\n",
        "    n_layers=4,\n",
        "    attn_heads=8,\n",
        "    dropout=0.1\n",
        ")\n",
        "def bert_forward(batch, model, device):\n",
        "    premise = batch[\"p_embed\"].to(device)\n",
        "    hypothesis = batch[\"h_embed\"].to(device)\n",
        "\n",
        "    p, h = batch_truncate(premise, hypothesis, max_len=128)\n",
        "    x, seg, attn_mask = batch_pad(p, h, max_len=128)\n",
        "\n",
        "    x = x.to(device)\n",
        "    seg = seg.to(device)\n",
        "    attn_mask = attn_mask.to(device)\n",
        "\n",
        "    outputs = model(x, seg, attn_mask)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "bert_task = Task(\n",
        "    model=bert_model,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    save_path=\"bert_nli.pth\",\n",
        "    optimizer=torch.optim.Adam(bert_model.parameters(), lr=0.001),\n",
        "    loss_fn=nn.BCEWithLogitsLoss(),\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    batch_size=8,\n",
        "    weighted_sampling=False,\n",
        "    forward_func=bert_forward\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|         | 163/2886 [00:40<11:20,  4.00it/s]"
          ]
        }
      ],
      "source": [
        "bert_task.fit(epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 163/163 [00:01<00:00, 85.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Overall Metrics ===\n",
            "Accuracy  : 0.6457\n",
            "Precision : 0.6555\n",
            "Recall    : 0.6256\n",
            "F1        : 0.6402\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'class_names' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m metrics = task.evaluate()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_metrcis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 265\u001b[39m, in \u001b[36mTask.visualize_metrcis\u001b[39m\u001b[34m(self, metrics_dict)\u001b[39m\n\u001b[32m    262\u001b[39m cm = metrics_dict[\u001b[33m\"\u001b[39m\u001b[33mconfusion_matrix\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    263\u001b[39m plt.figure(figsize=(\u001b[32m6\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m    264\u001b[39m sns.heatmap(cm, annot=\u001b[38;5;28;01mTrue\u001b[39;00m, fmt=\u001b[33m'\u001b[39m\u001b[33md\u001b[39m\u001b[33m'\u001b[39m, cmap=\u001b[33m'\u001b[39m\u001b[33mBlues\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m             xticklabels=\u001b[43mclass_names\u001b[49m, yticklabels=class_names)\n\u001b[32m    266\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mPredicted\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    267\u001b[39m plt.ylabel(\u001b[33m'\u001b[39m\u001b[33mActual\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'class_names' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "metrics = task.evaluate()\n",
        "task.visualize_metrcis(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
