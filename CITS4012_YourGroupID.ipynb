{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 CITS4012 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_path = \"dataset\"\n",
        "\n",
        "train_file = f\"{db_path}/train.json\"\n",
        "test_file = f\"{db_path}/test.json\"\n",
        "val_file = f\"{db_path}/train.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Pluto rotates once on its axis every 6.39 Earth days;',\n",
              " 'Earth rotates on its axis once times in one day.',\n",
              " 'neutral',\n",
              " 23088)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "with open(train_file, \"r\") as fp:\n",
        "    train = json.load(fp)\n",
        "with open(val_file, \"r\") as fp:\n",
        "    val = json.load(fp)\n",
        "with open(test_file, \"r\") as fp:\n",
        "    test = json.load(fp)\n",
        "\n",
        "train['premise']['0'], train['hypothesis']['0'], train['label']['0'], len(train['premise'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "from nltk import pos_tag\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Config:\n",
        "    stem = False\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatize = False\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenize = True\n",
        "\n",
        "    clean_text = True\n",
        "    regex = r\"[^a-z0-9\\-\\s]\"\n",
        "\n",
        "    pos = False\n",
        "    pos_tagger = pos_tag\n",
        "\n",
        "    lower = True\n",
        "\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "class textProcesser():\n",
        "    def __init__(self, cfg: Config) -> None:\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.lemmatizer = cfg.lemmatizer\n",
        "        self.stemmer = cfg.stemmer\n",
        "        self.stop_words = cfg.stopwords\n",
        "\n",
        "\n",
        "    def clean(self, sentence: str) -> list[str]:\n",
        "        tokens = [sentence]\n",
        "        if self.cfg.tokenize:\n",
        "            tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "        if self.cfg.lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.stem:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.clean_text:\n",
        "                \n",
        "            tokens = [re.sub(self.cfg.regex, '', token.lower() if self.cfg.lower else token) for token in tokens]\n",
        "            tokens = [token for token in tokens if token and token not in self.stop_words]\n",
        "\n",
        "        if self.cfg.pos:\n",
        "            tokens = self.cfg.pos_tagger(tokens)\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def clean_text(self, json):\n",
        "        clean = {'premise': {}, 'hypothesis': {}, 'label': {}}\n",
        "        for idx, key in enumerate(json['label'].keys()):\n",
        "            if 'neutral' in json['label'][key]:\n",
        "                clean['label'][idx] = 0\n",
        "            else:\n",
        "                clean['label'][idx] = 1\n",
        "\n",
        "            clean['premise'][idx] = self.clean(json['premise'][key])\n",
        "            clean['hypothesis'][idx] = self.clean(json['hypothesis'][key])\n",
        "            \n",
        "        return clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['geysers', '-', 'periodic', 'gush', 'hot', 'water', 'surface', 'earth'],\n",
              " ['surface', 'sun', 'much', 'hotter', 'almost', 'anything', 'earth'],\n",
              " 0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_train = textProcesser(Config()).clean_text(train)\n",
        "clean_val = textProcesser(Config()).clean_text(val)\n",
        "clean_test = textProcesser(Config()).clean_text(test)\n",
        "clean_train['premise'][2], clean_train['hypothesis'][2], clean_train['label'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[pluto, rotates, axis, every, 639, earth, days]</td>\n",
              "      <td>[earth, rotates, axis, times, one, day]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[--, -glenn, per, day, earth, rotates, axis]</td>\n",
              "      <td>[earth, rotates, axis, times, one, day]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[geysers, -, periodic, gush, hot, water, surfa...</td>\n",
              "      <td>[surface, sun, much, hotter, almost, anything,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[facts, liquid, water, droplets, changed, invi...</td>\n",
              "      <td>[evaporation, responsible, changing, liquid, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[comparison, earth, rotates, axis, per, day, r...</td>\n",
              "      <td>[earth, rotates, axis, times, one, day]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                            premise  \\\n",
              "0      0    [pluto, rotates, axis, every, 639, earth, days]   \n",
              "1      1       [--, -glenn, per, day, earth, rotates, axis]   \n",
              "2      0  [geysers, -, periodic, gush, hot, water, surfa...   \n",
              "3      1  [facts, liquid, water, droplets, changed, invi...   \n",
              "4      1  [comparison, earth, rotates, axis, per, day, r...   \n",
              "\n",
              "                                          hypothesis  \n",
              "0            [earth, rotates, axis, times, one, day]  \n",
              "1            [earth, rotates, axis, times, one, day]  \n",
              "2  [surface, sun, much, hotter, almost, anything,...  \n",
              "3  [evaporation, responsible, changing, liquid, w...  \n",
              "4            [earth, rotates, axis, times, one, day]  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    \"label\": clean_train[\"label\"],\n",
        "    \"premise\": clean_train[\"premise\"],\n",
        "    \"hypothesis\": clean_train[\"hypothesis\"]\n",
        "})\n",
        "\n",
        "# reset index if needed\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,  DataLoader\n",
        "\n",
        "class nliDataset(Dataset):\n",
        "  def __init__(self, df, embed_model):\n",
        "    self.df = df\n",
        "    self.embed_model = embed_model\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    premise = self.df[\"premise\"].iloc[index]\n",
        "    hypothesis = self.df[\"hypothesis\"].iloc[index]\n",
        "    p_embed = np.array([self.embed_model.wv[word] for word in premise if word in self.embed_model.wv])\n",
        "    h_embed = np.array([self.embed_model.wv[word] for word in hypothesis if word in self.embed_model.wv])\n",
        "    label = int(self.df[\"label\"].iloc[index])\n",
        "\n",
        "    p_embed = torch.tensor(p_embed, dtype=torch.float32)\n",
        "    h_embed = torch.tensor(h_embed, dtype=torch.float32)\n",
        "    label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    return p_embed, h_embed, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    p_embeds, h_embeds, labels = zip(*batch)\n",
        "\n",
        "    # Compute lengths for each sequence\n",
        "    p_lengths = torch.tensor([x.shape[0] for x in p_embeds], dtype=torch.long)\n",
        "    h_lengths = torch.tensor([x.shape[0] for x in h_embeds], dtype=torch.long)\n",
        "\n",
        "    p_padded = pad_sequence(p_embeds, batch_first=True)  # (batch, max_p_len, embed_dim)\n",
        "    h_padded = pad_sequence(h_embeds, batch_first=True)  # (batch, max_h_len, embed_dim)\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return p_padded, h_padded, labels, p_lengths, h_lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = list(clean_train['premise'].values()) + list(clean_train['hypothesis'].values())\n",
        "\n",
        "embed_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=3, workers=2, sg=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_db = nliDataset(train_df, embed_model=embed_model)\n",
        "loader = DataLoader(train_db, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class twin_LSTM(nn.Module):\n",
        "  def __init__(self, input_size=100, hidden_size=128, output_size=2, dropout=0.5, bidirectional=True):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.px_lstm = nn.LSTM(input_size, self.hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
        "    self.hx_lstm = nn.LSTM(input_size, self.hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(self.hidden_size * (2 if bidirectional else 1), output_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, px, hx, plengths, hlengths):\n",
        "\n",
        "    packed_px = pack_padded_sequence(px, plengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    px_output, (px_hn, _) = self.px_lstm(packed_px)\n",
        "\n",
        "    packed_hx = pack_padded_sequence(hx, hlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    hx_output, (hx_hn, _) = self.hx_lstm(packed_hx)\n",
        "\n",
        "    px_out = px_hn[-1]\n",
        "    hx_out = hx_hn[-1]\n",
        "\n",
        "\n",
        "    out = self.fc(torch.cat([px_out, hx_out], dim=1))\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 2])\n"
          ]
        }
      ],
      "source": [
        "twin = twin_LSTM(input_size=100, hidden_size=128, output_size=2, dropout=0.5, bidirectional=True)\n",
        "\n",
        "for batch in loader:\n",
        "    p_padded, h_padded, labels, p_lengths, h_lengths = batch\n",
        "    outputs = twin(p_padded, h_padded, p_lengths, h_lengths)\n",
        "    print(outputs.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
