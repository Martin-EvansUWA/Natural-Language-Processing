{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 CITS4012 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_path = \"dataset\"\n",
        "\n",
        "train_file = f\"{db_path}/train.json\"\n",
        "test_file = f\"{db_path}/test.json\"\n",
        "val_file = f\"{db_path}/validation.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Pluto rotates once on its axis every 6.39 Earth days;',\n",
              " 'Earth rotates on its axis once times in one day.',\n",
              " 'neutral',\n",
              " 23088)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "with open(train_file, \"r\") as fp:\n",
        "    train = json.load(fp)\n",
        "with open(val_file, \"r\") as fp:\n",
        "    val = json.load(fp)\n",
        "with open(test_file, \"r\") as fp:\n",
        "    test = json.load(fp)\n",
        "\n",
        "train['premise']['0'], train['hypothesis']['0'], train['label']['0'], len(train['premise'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "from nltk import pos_tag\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Config:\n",
        "    stem = False\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatize = True\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenize = True\n",
        "\n",
        "    clean_text = True\n",
        "    regex = r\"[^a-z0-9\\-\\s]\"\n",
        "\n",
        "    pos = True\n",
        "    pos_tagger = pos_tag\n",
        "\n",
        "    lower = True\n",
        "\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class textProcesser():\n",
        "    def __init__(self, cfg: Config) -> None:\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.lemmatizer = cfg.lemmatizer\n",
        "        self.stemmer = cfg.stemmer\n",
        "        self.stop_words = cfg.stopwords\n",
        "\n",
        "        self.pos_tagger = cfg.pos_tagger\n",
        "\n",
        "\n",
        "    def clean(self, sentence: str) -> list[str]:\n",
        "        tokens = [sentence]\n",
        "        if self.cfg.tokenize:\n",
        "            tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "        if self.cfg.lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.stem:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        if self.cfg.clean_text:\n",
        "                \n",
        "            tokens = [re.sub(self.cfg.regex, '', token.lower() if self.cfg.lower else token) for token in tokens]\n",
        "            tokens = [token for token in tokens if token and token not in self.stop_words]\n",
        "\n",
        "        if self.cfg.pos:\n",
        "            tokens_pos = pos_tag(tokens)\n",
        "            tokens, pos_tags = zip(*tokens_pos) if tokens_pos else ([], [])\n",
        "            return list(tokens), list(pos_tags)\n",
        "\n",
        "        return tokens, None\n",
        "    \n",
        "    def clean_text(self, json):\n",
        "        clean = {\n",
        "            'premise': {},\n",
        "            'premise_pos': {},\n",
        "            'hypothesis': {},\n",
        "            'hypothesis_pos': {},\n",
        "            'label': {}\n",
        "        }\n",
        "\n",
        "        for idx, key in enumerate(json['label'].keys()):\n",
        "            p_tokens, p_pos = self.clean(json['premise'][key])\n",
        "            h_tokens, h_pos = self.clean(json['hypothesis'][key])\n",
        "\n",
        "            if len(p_tokens) == 0 or len(h_tokens) == 0:\n",
        "                print(f\" Warning: Empty premise or hypothesis at index {idx} (key: {key}).\")\n",
        "\n",
        "            clean['label'][idx] = 0 if 'neutral' in json['label'][key] else 1\n",
        "            clean['premise'][idx] = p_tokens\n",
        "            clean['premise_pos'][idx] = p_pos\n",
        "            clean['hypothesis'][idx] = h_tokens\n",
        "            clean['hypothesis_pos'][idx] = h_pos\n",
        "\n",
        "        return clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Warning: Empty premise or hypothesis at index 146 (key: 146).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([],\n",
              " ['take', 'earth', 'one', 'week', 'rotate', 'axis', 'seven', 'time'],\n",
              " 0,\n",
              " [],\n",
              " ['VB', 'NN', 'CD', 'NN', 'NN', 'VBD', 'CD', 'NN'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cfg = Config()\n",
        "\n",
        "clean_train = textProcesser(cfg).clean_text(train)\n",
        "clean_val = textProcesser(cfg).clean_text(val)\n",
        "clean_test = textProcesser(cfg).clean_text(test)\n",
        "clean_train['premise'][146], clean_train['hypothesis'][146], clean_train['label'][146], clean_train['premise_pos'][146], clean_train['hypothesis_pos'][146]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>premise_pos</th>\n",
              "      <th>hypothesis_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[pluto, rotates, axis, every, 639, earth, day]</td>\n",
              "      <td>[earth, rotates, axis, time, one, day]</td>\n",
              "      <td>[NN, NNS, VBP, DT, CD, NN, NN]</td>\n",
              "      <td>[NN, VBZ, JJ, NN, CD, NN]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[--, -glenn, per, day, earth, rotates, axis]</td>\n",
              "      <td>[earth, rotates, axis, time, one, day]</td>\n",
              "      <td>[:, NN, IN, NN, NN, VBZ, IN]</td>\n",
              "      <td>[NN, VBZ, JJ, NN, CD, NN]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[geyser, -, periodic, gush, hot, water, surfac...</td>\n",
              "      <td>[surface, sun, much, hotter, almost, anything,...</td>\n",
              "      <td>[SYM, :, JJ, NN, JJ, NN, NN, NN]</td>\n",
              "      <td>[NN, NN, JJ, NN, RB, NN, JJ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[facts, liquid, water, droplet, changed, invis...</td>\n",
              "      <td>[evaporation, responsible, changing, liquid, w...</td>\n",
              "      <td>[NNS, JJ, NN, NN, VBD, JJ, NN, NN, NN, VBN, NN]</td>\n",
              "      <td>[NN, JJ, VBG, JJ, NN, NN, NN]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[comparison, earth, rotates, axis, per, day, r...</td>\n",
              "      <td>[earth, rotates, axis, time, one, day]</td>\n",
              "      <td>[NN, NN, VBZ, JJ, IN, NN, VBZ, IN, NN, IN, NN]</td>\n",
              "      <td>[NN, VBZ, JJ, NN, CD, NN]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                            premise  \\\n",
              "0      0     [pluto, rotates, axis, every, 639, earth, day]   \n",
              "1      1       [--, -glenn, per, day, earth, rotates, axis]   \n",
              "2      0  [geyser, -, periodic, gush, hot, water, surfac...   \n",
              "3      1  [facts, liquid, water, droplet, changed, invis...   \n",
              "4      1  [comparison, earth, rotates, axis, per, day, r...   \n",
              "\n",
              "                                          hypothesis  \\\n",
              "0             [earth, rotates, axis, time, one, day]   \n",
              "1             [earth, rotates, axis, time, one, day]   \n",
              "2  [surface, sun, much, hotter, almost, anything,...   \n",
              "3  [evaporation, responsible, changing, liquid, w...   \n",
              "4             [earth, rotates, axis, time, one, day]   \n",
              "\n",
              "                                       premise_pos  \\\n",
              "0                   [NN, NNS, VBP, DT, CD, NN, NN]   \n",
              "1                     [:, NN, IN, NN, NN, VBZ, IN]   \n",
              "2                 [SYM, :, JJ, NN, JJ, NN, NN, NN]   \n",
              "3  [NNS, JJ, NN, NN, VBD, JJ, NN, NN, NN, VBN, NN]   \n",
              "4   [NN, NN, VBZ, JJ, IN, NN, VBZ, IN, NN, IN, NN]   \n",
              "\n",
              "                  hypothesis_pos  \n",
              "0      [NN, VBZ, JJ, NN, CD, NN]  \n",
              "1      [NN, VBZ, JJ, NN, CD, NN]  \n",
              "2   [NN, NN, JJ, NN, RB, NN, JJ]  \n",
              "3  [NN, JJ, VBG, JJ, NN, NN, NN]  \n",
              "4      [NN, VBZ, JJ, NN, CD, NN]  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame({\n",
        "    \"label\": clean_train[\"label\"],\n",
        "    \"premise\": clean_train[\"premise\"],\n",
        "    \"hypothesis\": clean_train[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_train[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_train[\"hypothesis_pos\"]\n",
        "})\n",
        "\n",
        "val_df = pd.DataFrame({\n",
        "    \"label\": clean_val[\"label\"],\n",
        "    \"premise\": clean_val[\"premise\"],\n",
        "    \"hypothesis\": clean_val[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_val[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_val[\"hypothesis_pos\"]\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"label\": clean_test[\"label\"],\n",
        "    \"premise\": clean_test[\"premise\"],\n",
        "    \"hypothesis\": clean_test[\"hypothesis\"],\n",
        "    \"premise_pos\": clean_test[\"premise_pos\"],\n",
        "    \"hypothesis_pos\": clean_test[\"hypothesis_pos\"]\n",
        "})\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "splits = [('Train', train_df), ('Validation', val_df), ('Test', test_df)]\n",
        "colors = ['skyblue', 'salmon', 'lightgreen']  # one color per split\n",
        "\n",
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "for i, ((name, df), color) in enumerate(zip(splits, colors), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    counts = df['label'].value_counts().sort_index()\n",
        "    sns.barplot(x=['Neutral', 'Entails'], y=counts.values, color=color)\n",
        "    plt.title(f\"{name} Set\")\n",
        "    plt.xlabel(\"Label\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build pos2idx from training set only\n",
        "all_pos_tags = set()\n",
        "for pos_list in train_df['premise_pos']:\n",
        "    all_pos_tags.update(pos_list)\n",
        "for pos_list in train_df['hypothesis_pos']:\n",
        "    all_pos_tags.update(pos_list)\n",
        "\n",
        "# Add a special <UNK> token\n",
        "pos2idx = {pos: i for i, pos in enumerate(sorted(all_pos_tags))}\n",
        "pos2idx['<UNK>'] = len(pos2idx)\n",
        "num_pos_tags = len(pos2idx)\n",
        "\n",
        "def encode_pos(pos_list, pos2idx):\n",
        "    return [pos2idx.get(pos, pos2idx['<UNK>']) for pos in pos_list]\n",
        "\n",
        "\n",
        "len(pos2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,  DataLoader\n",
        "\n",
        "class nliDataset(Dataset):\n",
        "  def __init__(self, df, embed_model, pos_embed_model=None):\n",
        "    self.df = df\n",
        "    self.embed_model = embed_model\n",
        "    self.pos_encode = encode_pos\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    premise = self.df[\"premise\"].iloc[index]\n",
        "    hypothesis = self.df[\"hypothesis\"].iloc[index]\n",
        "    p_embed = np.array([self.embed_model.wv[word] for word in premise if word in self.embed_model.wv])\n",
        "    h_embed = np.array([self.embed_model.wv[word] for word in hypothesis if word in self.embed_model.wv])\n",
        "\n",
        "    if len(p_embed) == 0:\n",
        "        p_embed = np.zeros((1, self.embed_model.vector_size))\n",
        "    if len(h_embed) == 0:\n",
        "        h_embed = np.zeros((1, self.embed_model.vector_size))\n",
        "    \n",
        "\n",
        "    premise_pos = self.df[\"premise_pos\"].iloc[index]\n",
        "    hypothesis_pos = self.df[\"hypothesis_pos\"].iloc[index]\n",
        "\n",
        "    premise_pos = torch.tensor(self.pos_encode(premise_pos, pos2idx), dtype=torch.long)\n",
        "    hypothesis_pos = torch.tensor(self.pos_encode(hypothesis_pos, pos2idx), dtype=torch.long)\n",
        "\n",
        "\n",
        "    label = int(self.df[\"label\"].iloc[index])\n",
        "    p_embed = torch.tensor(p_embed, dtype=torch.float32)\n",
        "    h_embed = torch.tensor(h_embed, dtype=torch.float32)\n",
        "    label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
        "    \n",
        "    sample = {\n",
        "        \"p_embed\": p_embed,\n",
        "        \"h_embed\": h_embed,\n",
        "        \"label\": label,\n",
        "        \"premise_pos\": premise_pos,\n",
        "        \"hypothesis_pos\": hypothesis_pos\n",
        "    }\n",
        "\n",
        "    return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    p_embeds = [item[\"p_embed\"] for item in batch]\n",
        "    h_embeds = [item[\"h_embed\"] for item in batch]\n",
        "    p_pos_idx = [item[\"premise_pos\"] for item in batch]\n",
        "    h_pos_idx = [item[\"hypothesis_pos\"] for item in batch]\n",
        "    labels = [item[\"label\"] for item in batch]\n",
        "\n",
        "    # Get max lengths from word embeddings\n",
        "    max_len_px = max([x.shape[0] for x in p_embeds])\n",
        "    max_len_hx = max([x.shape[0] for x in h_embeds])\n",
        "\n",
        "    # Pad word embeddings\n",
        "    p_padded = pad_sequence(p_embeds, batch_first=True)  # shape: [B, max_len_px, emb_dim]\n",
        "    h_padded = pad_sequence(h_embeds, batch_first=True)  # shape: [B, max_len_hx, emb_dim]\n",
        "\n",
        "    # Pad POS indices **to match word embeddings**\n",
        "    p_pos_padded = torch.stack([\n",
        "        torch.cat([x, torch.zeros(max_len_px - x.shape[0], dtype=torch.long)]) if x.shape[0] < max_len_px else x\n",
        "        for x in p_pos_idx\n",
        "    ])\n",
        "    h_pos_padded = torch.stack([\n",
        "        torch.cat([x, torch.zeros(max_len_hx - x.shape[0], dtype=torch.long)]) if x.shape[0] < max_len_hx else x\n",
        "        for x in h_pos_idx\n",
        "    ])\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "    plengths = torch.tensor([x.shape[0] for x in p_embeds])\n",
        "    hlengths = torch.tensor([x.shape[0] for x in h_embeds])\n",
        "\n",
        "    return {\n",
        "        \"p_embed\": p_padded,\n",
        "        \"h_embed\": h_padded,\n",
        "        \"p_pos\": p_pos_padded,\n",
        "        \"h_pos\": h_pos_padded,\n",
        "        \"label\": labels,\n",
        "        \"p_lengths\": plengths,\n",
        "        \"h_lengths\": hlengths\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = list(clean_train['premise'].values()) + list(clean_train['hypothesis'].values())\n",
        "embed_model = Word2Vec(sentences=sentences, vector_size=200, window=10, min_count=3, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2886, 163, 266)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_db = nliDataset(train_df, embed_model=embed_model)\n",
        "trainloader = DataLoader(train_db, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_db = nliDataset(val_df, embed_model=embed_model)\n",
        "valloader = DataLoader(val_db, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "test_db = nliDataset(test_df, embed_model=embed_model)\n",
        "testloader = DataLoader(test_db, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "len(trainloader), len(valloader), len(testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Task():\n",
        "  def __init__(self, model, \n",
        "                train_df, \n",
        "                val_df, \n",
        "                save_path, \n",
        "                collate_fn=collate_fn, \n",
        "                optimizer=None,\n",
        "                loss_fn=None,\n",
        "                device=\"cpu\", \n",
        "                batch_size=16, \n",
        "                weighted_sampling=False) -> None:\n",
        "\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "    self.save_path = save_path\n",
        "\n",
        "    if weighted_sampling:\n",
        "      labels = train_df['label'].values\n",
        "      class_counts = Counter(labels)\n",
        "      num_samples = len(labels)\n",
        "      class_weights = {cls: num_samples/count for cls, count in class_counts.items()}\n",
        "      sample_weights = [class_weights[label] for label in labels]\n",
        "      sampler = WeightedRandomSampler(sample_weights, num_samples=num_samples, replacement=True)\n",
        "    else:\n",
        "      sampler = None\n",
        "    \n",
        "    train_db = nliDataset(train_df, embed_model=embed_model)\n",
        "    val_db = nliDataset(val_df, embed_model=embed_model)\n",
        "\n",
        "    self.train_loader = DataLoader(train_db, batch_size=batch_size, sampler=sampler, shuffle=True, collate_fn=collate_fn)\n",
        "    self.val_loader = DataLoader(val_db, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    self.model = model\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "    self.model = model.to(device)\n",
        "\n",
        "    self.losses = {}\n",
        "    self.losses[\"train\"] = []\n",
        "    self.losses[\"val\"] = []\n",
        "\n",
        "  def train_epoch(self):\n",
        "    self.model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(self.train_loader, total=len(self.train_loader), desc=\"Training\"):\n",
        "        \n",
        "        p_padded = batch[\"p_embed\"]\n",
        "        h_padded = batch[\"h_embed\"]\n",
        "        labels = batch[\"label\"]\n",
        "        p_lengths = batch[\"p_lengths\"]\n",
        "        h_lengths = batch[\"h_lengths\"]\n",
        "\n",
        "        premise_pos = batch[\"p_pos\"]\n",
        "        hypothesis_pos = batch[\"h_pos\"]\n",
        "      \n",
        "\n",
        "        p_padded = p_padded.to(self.device)\n",
        "        h_padded = h_padded.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        outputs = self.model(p_padded, h_padded, p_lengths, h_lengths, premise_pos, hypothesis_pos)\n",
        "\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(self.train_loader)\n",
        "    return epoch_loss\n",
        "  \n",
        "\n",
        "  def validate_epoch(self):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, total=len(self.val_loader), desc=\"Validating\"):\n",
        "            \n",
        "          p_padded = batch[\"p_embed\"]\n",
        "          h_padded = batch[\"h_embed\"]\n",
        "          labels = batch[\"label\"]\n",
        "          p_lengths = batch[\"p_lengths\"]\n",
        "          h_lengths = batch[\"h_lengths\"]\n",
        "\n",
        "          premise_pos = batch[\"p_pos\"]\n",
        "          hypothesis_pos = batch[\"h_pos\"]\n",
        "        \n",
        "\n",
        "          p_padded = p_padded.to(self.device)\n",
        "          h_padded = h_padded.to(self.device)\n",
        "          labels = labels.to(self.device)\n",
        "\n",
        "          outputs = self.model(p_padded, h_padded, p_lengths, h_lengths, premise_pos, hypothesis_pos)\n",
        "          loss = self.loss_fn(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "          correct += (predicted == labels.squeeze()).sum().item()\n",
        "          total += labels.size(0)\n",
        "\n",
        "        val_loss /= len(self.val_loader)\n",
        "        accuracy = correct / total\n",
        "    return val_loss, accuracy\n",
        "     \n",
        "\n",
        "  def fit(self, epochs, restore_best=True):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = self.train_epoch()\n",
        "        val_loss, val_acc = self.validate_epoch()\n",
        "\n",
        "        self.losses[\"train\"].append(train_loss)\n",
        "        self.losses[\"val\"].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(self.model.state_dict(), self.save_path)\n",
        "            print(f\"Model saved to {self.save_path}\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    if restore_best:\n",
        "      print(\"Restoring best model weights.\")\n",
        "      self.model.load_state_dict(torch.load(self.save_path))\n",
        "    return self.losses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class twin_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=100, hidden_size=128, output_size=1, \n",
        "                 dropout=0.5, bidirectional=True, n_layers=3,\n",
        "                 pos_vocab_size=None, pos_emb_dim=16):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Word + POS embeddings combined\n",
        "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_emb_dim) if pos_vocab_size else None\n",
        "        combined_input_size = input_size + (pos_emb_dim if pos_vocab_size else 0)\n",
        "\n",
        "        self.px_lstm = nn.LSTM(combined_input_size, self.hidden_size, \n",
        "                               num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hx_lstm = nn.LSTM(combined_input_size, self.hidden_size, \n",
        "                               num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size * 2 * (2 if bidirectional else 1), output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, px, hx, plengths=None, hlengths=None, p_pos_idx=None, h_pos_idx=None):\n",
        "        # Add POS embeddings if provided\n",
        "        if self.pos_embedding is not None and p_pos_idx is not None and h_pos_idx is not None:\n",
        "            print(\"POS embeddings added.\")\n",
        "            p_pos_emb = self.pos_embedding(p_pos_idx)\n",
        "            h_pos_emb = self.pos_embedding(h_pos_idx)\n",
        "\n",
        "            print(f\"p_pos_emb shape: {p_pos_emb.shape}, h_pos_emb shape: {h_pos_emb.shape}\")\n",
        "            print(f\"px shape before concat: {px.shape}, hx shape before concat: {hx.shape}\")\n",
        "\n",
        "            px = torch.cat([px, p_pos_emb], dim=2)\n",
        "            hx = torch.cat([hx, h_pos_emb], dim=2)\n",
        "\n",
        "        # Pack sequences\n",
        "        packed_px = pack_padded_sequence(px, plengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        px_output, (px_hn, _) = self.px_lstm(packed_px)\n",
        "\n",
        "        packed_hx = pack_padded_sequence(hx, hlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        hx_output, (hx_hn, _) = self.hx_lstm(packed_hx)\n",
        "\n",
        "        # Take last hidden state(s)\n",
        "        if self.bidirectional:\n",
        "            px_out = torch.cat([px_hn[-2], px_hn[-1]], dim=1)\n",
        "            hx_out = torch.cat([hx_hn[-2], hx_hn[-1]], dim=1)\n",
        "        else:\n",
        "            px_out = px_hn[-1]\n",
        "            hx_out = hx_hn[-1]\n",
        "\n",
        "        out = self.fc(torch.cat([px_out, hx_out], dim=1))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pos_vocab_size = len(pos2idx) \n",
        "pos_emb_dim = 32              \n",
        "\n",
        "model = twin_LSTM(\n",
        "    input_size=embed_model.vector_size,\n",
        "    hidden_size=128,\n",
        "    output_size=1,\n",
        "    dropout=0.5,\n",
        "    bidirectional=True,\n",
        "    n_layers=2,\n",
        "    pos_vocab_size=pos_vocab_size,\n",
        "    pos_emb_dim=pos_emb_dim\n",
        ")\n",
        "\n",
        "task = Task(\n",
        "    model=model,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    save_path=\"best_model.pth\",\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
        "    loss_fn=nn.BCEWithLogitsLoss(),\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    batch_size=8,\n",
        "    weighted_sampling=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating:   0%|          | 0/163 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS embeddings added.\n",
            "p_pos_emb shape: torch.Size([8, 10, 32]), h_pos_emb shape: torch.Size([8, 8, 32])\n",
            "px shape before concat: torch.Size([8, 10, 200]), hx shape before concat: torch.Size([8, 8, 200])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 200 but got size 32 for tensor number 1 in the list.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mTask.validate_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    104\u001b[39m h_padded = h_padded.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    105\u001b[39m labels = labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpremise_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_fn(outputs, labels)\n\u001b[32m    109\u001b[39m val_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtwin_LSTM.forward\u001b[39m\u001b[34m(self, px, hx, plengths, hlengths, p_pos_idx, h_pos_idx)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mp_pos_emb shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_pos_emb.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, h_pos_emb shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_pos_emb.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpx shape before concat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, hx shape before concat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     px = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_pos_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     hx = torch.cat([hx, h_pos_emb], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Pack sequences\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 200 but got size 32 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "task.validate_epoch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|█         | 298/2886 [01:51<16:09,  2.67it/s]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mTask.fit\u001b[39m\u001b[34m(self, epochs, restore_best)\u001b[39m\n\u001b[32m    122\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     val_loss, val_acc = \u001b[38;5;28mself\u001b[39m.validate_epoch()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m.losses[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mTask.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_fn(outputs, labels)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     78\u001b[39m epoch_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Natural-Language-Processing\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "task.fit(epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
